{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6263e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "download(\"punkt_tab\")\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7514567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vocabulary Size (including SOS/EOS/UNK): 4731\n",
      "Total Patterns:  2277\n",
      "Epoch [1/100], Training Loss: 8.1642\n",
      "Epoch [1/100], Validation Loss: 6.9251\n",
      "Epoch [2/100], Training Loss: 5.7891\n",
      "Epoch [2/100], Validation Loss: 5.7248\n",
      "Epoch [3/100], Training Loss: 5.2696\n",
      "Epoch [3/100], Validation Loss: 5.6236\n",
      "Epoch [4/100], Training Loss: 5.0906\n",
      "Epoch [4/100], Validation Loss: 5.5547\n",
      "Epoch [5/100], Training Loss: 4.9615\n",
      "Epoch [5/100], Validation Loss: 5.5078\n",
      "Epoch [6/100], Training Loss: 4.7986\n",
      "Epoch [6/100], Validation Loss: 5.4780\n",
      "Epoch [7/100], Training Loss: 4.6620\n",
      "Epoch [7/100], Validation Loss: 5.4298\n",
      "Epoch [8/100], Training Loss: 4.5271\n",
      "Epoch [8/100], Validation Loss: 5.4425\n",
      "Epoch [9/100], Training Loss: 4.3979\n",
      "Epoch [9/100], Validation Loss: 5.4396\n",
      "Epoch [10/100], Training Loss: 4.2769\n",
      "Epoch [10/100], Validation Loss: 5.4358\n",
      "Epoch [11/100], Training Loss: 4.1448\n",
      "Epoch [11/100], Validation Loss: 5.3565\n",
      "Epoch [12/100], Training Loss: 3.9952\n",
      "Epoch [12/100], Validation Loss: 5.3985\n",
      "Epoch [13/100], Training Loss: 3.8396\n",
      "Epoch [13/100], Validation Loss: 5.3595\n",
      "Epoch [14/100], Training Loss: 3.7287\n",
      "Epoch [14/100], Validation Loss: 5.3553\n",
      "Epoch [15/100], Training Loss: 3.6212\n",
      "Epoch [15/100], Validation Loss: 5.3831\n",
      "Epoch [16/100], Training Loss: 3.4676\n",
      "Epoch [16/100], Validation Loss: 5.3897\n",
      "Epoch [17/100], Training Loss: 3.3284\n",
      "Epoch [17/100], Validation Loss: 5.4096\n",
      "Epoch [18/100], Training Loss: 3.1766\n",
      "Epoch [18/100], Validation Loss: 5.3635\n",
      "Epoch [19/100], Training Loss: 3.0601\n",
      "Epoch [19/100], Validation Loss: 5.3981\n",
      "Epoch [20/100], Training Loss: 2.9508\n",
      "Epoch [20/100], Validation Loss: 5.3882\n",
      "Epoch [21/100], Training Loss: 2.8084\n",
      "Epoch [21/100], Validation Loss: 5.4058\n",
      "Epoch [22/100], Training Loss: 2.7077\n",
      "Epoch [22/100], Validation Loss: 5.3994\n",
      "Epoch [23/100], Training Loss: 2.5843\n",
      "Epoch [23/100], Validation Loss: 5.4243\n",
      "Epoch [24/100], Training Loss: 2.4654\n",
      "Epoch [24/100], Validation Loss: 5.4447\n",
      "Epoch [25/100], Training Loss: 2.3636\n",
      "Epoch [25/100], Validation Loss: 5.4764\n",
      "Epoch [26/100], Training Loss: 2.2391\n",
      "Epoch [26/100], Validation Loss: 5.5016\n",
      "Epoch [27/100], Training Loss: 2.1418\n",
      "Epoch [27/100], Validation Loss: 5.5169\n",
      "Epoch [28/100], Training Loss: 2.0331\n",
      "Epoch [28/100], Validation Loss: 5.5230\n",
      "Epoch [29/100], Training Loss: 1.9396\n",
      "Epoch [29/100], Validation Loss: 5.5489\n",
      "Epoch [30/100], Training Loss: 1.8231\n",
      "Epoch [30/100], Validation Loss: 5.5723\n",
      "Epoch [31/100], Training Loss: 1.7731\n",
      "Epoch [31/100], Validation Loss: 5.5469\n",
      "Epoch [32/100], Training Loss: 1.6679\n",
      "Epoch [32/100], Validation Loss: 5.5832\n",
      "Epoch [33/100], Training Loss: 1.6103\n",
      "Epoch [33/100], Validation Loss: 5.6214\n",
      "Epoch [34/100], Training Loss: 1.5275\n",
      "Epoch [34/100], Validation Loss: 5.6354\n",
      "Epoch [35/100], Training Loss: 1.4614\n",
      "Epoch [35/100], Validation Loss: 5.6236\n",
      "Epoch [36/100], Training Loss: 1.3660\n",
      "Epoch [36/100], Validation Loss: 5.6563\n",
      "Epoch [37/100], Training Loss: 1.3162\n",
      "Epoch [37/100], Validation Loss: 5.6148\n",
      "Epoch [38/100], Training Loss: 1.2619\n",
      "Epoch [38/100], Validation Loss: 5.6738\n",
      "Epoch [39/100], Training Loss: 1.1930\n",
      "Epoch [39/100], Validation Loss: 5.6750\n",
      "Epoch [40/100], Training Loss: 1.1614\n",
      "Epoch [40/100], Validation Loss: 5.7311\n",
      "Epoch [41/100], Training Loss: 1.0903\n",
      "Epoch [41/100], Validation Loss: 5.7553\n",
      "Epoch [42/100], Training Loss: 1.0202\n",
      "Epoch [42/100], Validation Loss: 5.6796\n",
      "Epoch [43/100], Training Loss: 0.9967\n",
      "Epoch [43/100], Validation Loss: 5.7895\n",
      "Epoch [44/100], Training Loss: 0.9536\n",
      "Epoch [44/100], Validation Loss: 5.7559\n",
      "Epoch [45/100], Training Loss: 0.9257\n",
      "Epoch [45/100], Validation Loss: 5.7823\n",
      "Epoch [46/100], Training Loss: 0.8694\n",
      "Epoch [46/100], Validation Loss: 5.8288\n",
      "Epoch [47/100], Training Loss: 0.8301\n",
      "Epoch [47/100], Validation Loss: 5.8018\n",
      "Epoch [48/100], Training Loss: 0.8056\n",
      "Epoch [48/100], Validation Loss: 5.8317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    125\u001b[39m     optimizer.zero_grad()\n\u001b[32m    126\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     train_loss_epoch += loss.item()\n\u001b[32m    129\u001b[39m avg_train_loss = train_loss_epoch / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    211\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    213\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    214\u001b[39m         group,\n\u001b[32m    215\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         state_steps,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    782\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:432\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    430\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, download # Ensure nltk is imported\n",
    "\n",
    "# It's good practice to call nltk.download() at a more global scope or ensure it's run once.\n",
    "# These lines are already in your file, ensure 'punkt' is available for word_tokenize.\n",
    "download(\"punkt_tab\") \n",
    "download(\"stopwords\") # Not used in this specific change but kept from original\n",
    "\n",
    "df = pd.read_csv('../stage_4_data/text_generation/data.csv') # I changed it to data.csv\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out) # Apply dropout to the last time step\n",
    "        out = self.fc(out[:, -1, :]) # Get output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define special tokens\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\" # Token for unknown words\n",
    "\n",
    "# Get jokes from DataFrame\n",
    "raw_jokes = df['Joke'].astype(str).tolist()\n",
    "\n",
    "# Create word vocabulary\n",
    "all_words_for_vocab = []\n",
    "for joke_str in raw_jokes:\n",
    "    tokens = word_tokenize(joke_str.lower()) # Tokenize and convert to lowercase\n",
    "    all_words_for_vocab.extend(tokens)\n",
    "\n",
    "unique_words_from_data = sorted(list(set(all_words_for_vocab)))\n",
    "\n",
    "# Add special tokens to the word list\n",
    "words = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + unique_words_from_data # Changed from chars\n",
    "word_to_int = {word: i for i, word in enumerate(words)} # Changed from char_to_int\n",
    "int_to_word = {i: word for i, word in enumerate(words)} # Changed from int_to_char\n",
    "\n",
    "vocab_size = len(words) # Changed from len(chars)\n",
    "print(f\"Word Vocabulary Size (including SOS/EOS/UNK): {vocab_size}\")\n",
    "\n",
    "embedding_dim = 64 # tunable\n",
    "hidden_size = 128  # tunable\n",
    "output_size = vocab_size # Output size must match new vocab_size\n",
    "learning_rate = 0.001\n",
    "sequence_length = 25 # Length of input sequences (now in words)\n",
    "epochs = 100 # Number of epochs\n",
    "dropout_probability = 0.5\n",
    "\n",
    "# Prepare training data\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for joke_str in raw_jokes:\n",
    "    # Tokenize joke into words, add SOS/EOS, and convert to lowercase\n",
    "    tokenized_joke_words = [SOS_TOKEN] + word_tokenize(joke_str.lower()) + [EOS_TOKEN]\n",
    "    \n",
    "    # Convert words to integers, using UNK_TOKEN for out-of-vocabulary words\n",
    "    # (though all words from raw_jokes should be in vocab here as it's built from them)\n",
    "    int_joke = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_joke_words]\n",
    "    \n",
    "    # Create sequences from this single joke\n",
    "    # A sequence needs at least sequence_length + 1 tokens (words) to form one input-output pair\n",
    "    if len(int_joke) > sequence_length:\n",
    "        for i in range(0, len(int_joke) - sequence_length, 1):\n",
    "            seq_in = int_joke[i : i + sequence_length]\n",
    "            seq_out = int_joke[i + sequence_length] # The target word (or EOS_TOKEN)\n",
    "            dataX.append(seq_in)\n",
    "            dataY.append(seq_out)\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns) # This will likely change\n",
    "\n",
    "split_idx = int(n_patterns * 0.8)\n",
    "\n",
    "train_dataX = dataX[:split_idx]\n",
    "train_dataY = dataY[:split_idx]\n",
    "test_dataX = dataX[split_idx:]\n",
    "test_dataY = dataY[split_idx:]\n",
    "\n",
    "X_train = torch.tensor(train_dataX, dtype=torch.long)\n",
    "Y_train = torch.tensor(train_dataY, dtype=torch.long)\n",
    "X_test = torch.tensor(test_dataX, dtype=torch.long)\n",
    "Y_test = torch.tensor(test_dataY, dtype=torch.long)\n",
    "\n",
    "# Instantiate the model - this will now use the updated vocab_size\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=dropout_probability)\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# The training loop would typically be in your main script (e.g., main.ipynb)\n",
    "# or called from there if this helper.py is imported.\n",
    "# For completeness, if you intend to run training directly from this script,\n",
    "# you would add the training loop here:\n",
    "#\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            loss_val = criterion(outputs_val, batch_y_val)\n",
    "            val_loss += loss_val.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ba957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10289, 10289)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jlen(dataX), len(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ecad334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \"what do you\"\n",
      "Generated text: \n",
      "--------------------------\n",
      "what do you call boat see weird and anakin funnier to kill it .  <EOS>\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Generation complete.\n",
      "Full generated joke:  what do you call boat see weird and anakin funnier to kill it .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Parameters for generation\n",
    "start_string = \"What do you\" # Or any other seed text\n",
    "num_words_to_generate = 50 # Maximum number of words to generate\n",
    "temperature = 0.8 # Higher temperature results in more random, lower in more predictable text\n",
    "\n",
    "# Tokenize the start string and convert to integers\n",
    "tokenized_start_string = word_tokenize(start_string.lower())\n",
    "pattern = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_start_string]\n",
    "\n",
    "if not pattern:\n",
    "    print(f\"Error: Seed string '{start_string}' contains no known words or is too short after tokenization.\")\n",
    "    # Fallback to a default known pattern if the seed is problematic\n",
    "    # Using SOS_TOKEN as a starting point if the original start_string is empty or all unknown\n",
    "    pattern = [word_to_int[SOS_TOKEN]] * min(sequence_length, 5) # Use a short sequence of SOS_TOKEN\n",
    "    # Reconstruct start_string for printing purposes, though it might just be SOS tokens\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "    print(f\"Using fallback seed: '{start_string_for_print}'\")\n",
    "else:\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "\n",
    "\n",
    "generated_words = [int_to_word.get(p, UNK_TOKEN) for p in pattern] # Store generated words\n",
    "\n",
    "print(f\"Seed: \\\"{start_string_for_print}\\\"\")\n",
    "print(\"Generated text: \")\n",
    "print(\"--------------------------\")\n",
    "print(start_string_for_print, end=\" \") # Print with a space at the end\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for i in range(num_words_to_generate):\n",
    "        # Ensure the pattern is of the correct sequence_length for input\n",
    "        # If pattern is shorter than sequence_length, pad with UNK_TOKEN (or SOS_TOKEN)\n",
    "        # For simplicity, we'll just take the last part if it's long enough,\n",
    "        # or the whole pattern if it's shorter.\n",
    "        # The model expects sequence_length inputs.\n",
    "        \n",
    "        current_sequence_input_indices = pattern[-sequence_length:]\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = torch.tensor([current_sequence_input_indices], dtype=torch.long)\n",
    "        # input_tensor = input_tensor.to(device) # Uncomment if using GPU\n",
    "\n",
    "        # Get model output (logits)\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Apply temperature to logits\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        # Sample from the distribution\n",
    "        predicted_word_index = torch.multinomial(output_dist, 1)[0].item()\n",
    "        \n",
    "        # Get the predicted word\n",
    "        predicted_word = int_to_word.get(predicted_word_index, UNK_TOKEN)\n",
    "\n",
    "        if predicted_word == EOS_TOKEN:\n",
    "            print(f\" {EOS_TOKEN}\") # Print EOS token\n",
    "            break # Stop generation if EOS token is predicted\n",
    "        \n",
    "        generated_words.append(predicted_word)\n",
    "        pattern.append(predicted_word_index)\n",
    "        print(predicted_word, end=\" \", flush=True) # Print word with a space\n",
    "        \n",
    "        # Slide the window: ensure pattern doesn't grow indefinitely beyond what's needed for context\n",
    "        if len(pattern) > sequence_length:\n",
    "            pattern = pattern[1:]\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------\")\n",
    "print(\"\\nGeneration complete.\")\n",
    "\n",
    "# Full generated text:\n",
    "print(\"Full generated joke: \", \" \".join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730261d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "189G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
