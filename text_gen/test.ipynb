{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6263e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reetibandyopadhyay/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reetibandyopadhyay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "download(\"punkt_tab\")\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7514567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reetibandyopadhyay/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reetibandyopadhyay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What did the bartender say to the jumper cable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't you hate jokes about German sausage? The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Two artists had an art contest... It ended in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Why did the chicken cross the playground? To g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What gun do you use to hunt a moose? A moosecut!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  What did the bartender say to the jumper cable...\n",
       "1   2  Don't you hate jokes about German sausage? The...\n",
       "2   3  Two artists had an art contest... It ended in ...\n",
       "3   4  Why did the chicken cross the playground? To g...\n",
       "4   5   What gun do you use to hunt a moose? A moosecut!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, download # Ensure nltk is imported\n",
    "\n",
    "# It's good practice to call nltk.download() at a more global scope or ensure it's run once.\n",
    "# These lines are already in your file, ensure 'punkt' is available for word_tokenize.\n",
    "download(\"punkt_tab\") \n",
    "download(\"stopwords\") # Not used in this specific change but kept from original\n",
    "\n",
    "#df = pd.read_csv('../stage_4_data/text_generation/data.csv') # I changed it to data.csv\n",
    "df = pd.read_csv('../stage_4_data/data.csv') # just for my directory structure\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fb6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vocabulary Size (including SOS/EOS/UNK): 4731\n",
      "Total Patterns:  8922\n",
      "Epoch [1/100], Training Loss: 8.3982\n",
      "Epoch [2/100], Training Loss: 7.3691\n",
      "Epoch [3/100], Training Loss: 6.0078\n",
      "Epoch [4/100], Training Loss: 5.6952\n",
      "Epoch [5/100], Training Loss: 5.6167\n",
      "Epoch [6/100], Training Loss: 5.5570\n",
      "Epoch [7/100], Training Loss: 5.5162\n",
      "Epoch [8/100], Training Loss: 5.4697\n",
      "Epoch [9/100], Training Loss: 5.4354\n",
      "Epoch [10/100], Training Loss: 5.4109\n",
      "Epoch [11/100], Training Loss: 5.3758\n",
      "Epoch [12/100], Training Loss: 5.3535\n",
      "Epoch [13/100], Training Loss: 5.3260\n",
      "Epoch [14/100], Training Loss: 5.2968\n",
      "Epoch [15/100], Training Loss: 5.2699\n",
      "Epoch [16/100], Training Loss: 5.2327\n",
      "Epoch [17/100], Training Loss: 5.2167\n",
      "Epoch [18/100], Training Loss: 5.1806\n",
      "Epoch [19/100], Training Loss: 5.1653\n",
      "Epoch [20/100], Training Loss: 5.1303\n",
      "Epoch [21/100], Training Loss: 5.0983\n",
      "Epoch [22/100], Training Loss: 5.0811\n",
      "Epoch [23/100], Training Loss: 5.0614\n",
      "Epoch [24/100], Training Loss: 5.0222\n",
      "Epoch [25/100], Training Loss: 5.0059\n",
      "Epoch [26/100], Training Loss: 4.9734\n",
      "Epoch [27/100], Training Loss: 4.9476\n",
      "Epoch [28/100], Training Loss: 4.9284\n",
      "Epoch [29/100], Training Loss: 4.8946\n",
      "Epoch [30/100], Training Loss: 4.8750\n",
      "Epoch [31/100], Training Loss: 4.8492\n",
      "Epoch [32/100], Training Loss: 4.8148\n",
      "Epoch [33/100], Training Loss: 4.7937\n",
      "Epoch [34/100], Training Loss: 4.7681\n",
      "Epoch [35/100], Training Loss: 4.7562\n",
      "Epoch [36/100], Training Loss: 4.7271\n",
      "Epoch [37/100], Training Loss: 4.7094\n",
      "Epoch [38/100], Training Loss: 4.6705\n",
      "Epoch [39/100], Training Loss: 4.6556\n",
      "Epoch [40/100], Training Loss: 4.6275\n",
      "Epoch [41/100], Training Loss: 4.6042\n",
      "Epoch [42/100], Training Loss: 4.5932\n",
      "Epoch [43/100], Training Loss: 4.5525\n",
      "Epoch [44/100], Training Loss: 4.5226\n",
      "Epoch [45/100], Training Loss: 4.5121\n",
      "Epoch [46/100], Training Loss: 4.4877\n",
      "Epoch [47/100], Training Loss: 4.4634\n",
      "Epoch [48/100], Training Loss: 4.4440\n",
      "Epoch [49/100], Training Loss: 4.4227\n",
      "Epoch [50/100], Training Loss: 4.4004\n",
      "Epoch [51/100], Training Loss: 4.3850\n",
      "Epoch [52/100], Training Loss: 4.3503\n",
      "Epoch [53/100], Training Loss: 4.3337\n",
      "Epoch [54/100], Training Loss: 4.3277\n",
      "Epoch [55/100], Training Loss: 4.2914\n",
      "Epoch [56/100], Training Loss: 4.2687\n",
      "Epoch [57/100], Training Loss: 4.2532\n",
      "Epoch [58/100], Training Loss: 4.2254\n",
      "Epoch [59/100], Training Loss: 4.2125\n",
      "Epoch [60/100], Training Loss: 4.1938\n",
      "Epoch [61/100], Training Loss: 4.1684\n",
      "Epoch [62/100], Training Loss: 4.1478\n",
      "Epoch [63/100], Training Loss: 4.1176\n",
      "Epoch [64/100], Training Loss: 4.1128\n",
      "Epoch [65/100], Training Loss: 4.0847\n",
      "Epoch [66/100], Training Loss: 4.0640\n",
      "Epoch [67/100], Training Loss: 4.0465\n",
      "Epoch [68/100], Training Loss: 4.0340\n",
      "Epoch [69/100], Training Loss: 4.0088\n",
      "Epoch [70/100], Training Loss: 3.9945\n",
      "Epoch [71/100], Training Loss: 3.9807\n",
      "Epoch [72/100], Training Loss: 3.9645\n",
      "Epoch [73/100], Training Loss: 3.9362\n",
      "Epoch [74/100], Training Loss: 3.9196\n",
      "Epoch [75/100], Training Loss: 3.9120\n",
      "Epoch [76/100], Training Loss: 3.8761\n",
      "Epoch [77/100], Training Loss: 3.8706\n",
      "Epoch [78/100], Training Loss: 3.8411\n",
      "Epoch [79/100], Training Loss: 3.8352\n",
      "Epoch [80/100], Training Loss: 3.8178\n",
      "Epoch [81/100], Training Loss: 3.7840\n",
      "Epoch [82/100], Training Loss: 3.7769\n",
      "Epoch [83/100], Training Loss: 3.7646\n",
      "Epoch [84/100], Training Loss: 3.7389\n",
      "Epoch [85/100], Training Loss: 3.7104\n",
      "Epoch [86/100], Training Loss: 3.7073\n",
      "Epoch [87/100], Training Loss: 3.6891\n",
      "Epoch [88/100], Training Loss: 3.6644\n",
      "Epoch [89/100], Training Loss: 3.6618\n",
      "Epoch [90/100], Training Loss: 3.6372\n",
      "Epoch [91/100], Training Loss: 3.6150\n",
      "Epoch [92/100], Training Loss: 3.6051\n",
      "Epoch [93/100], Training Loss: 3.5881\n",
      "Epoch [94/100], Training Loss: 3.5728\n",
      "Epoch [95/100], Training Loss: 3.5589\n",
      "Epoch [96/100], Training Loss: 3.5387\n",
      "Epoch [97/100], Training Loss: 3.5171\n",
      "Epoch [98/100], Training Loss: 3.5214\n",
      "Epoch [99/100], Training Loss: 3.4892\n",
      "Epoch [100/100], Training Loss: 3.4792\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        #self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out) # Apply dropout to the last time step\n",
    "        out = self.fc(out[:, -1, :]) # Get output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define special tokens\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\" # Token for unknown words\n",
    "\n",
    "# Get jokes from DataFrame\n",
    "raw_jokes = df['Joke'].astype(str).tolist()\n",
    "\n",
    "# Create word vocabulary\n",
    "all_words_for_vocab = []\n",
    "for joke_str in raw_jokes:\n",
    "    tokens = word_tokenize(joke_str.lower()) # Tokenize and convert to lowercase\n",
    "    all_words_for_vocab.extend(tokens)\n",
    "\n",
    "unique_words_from_data = sorted(list(set(all_words_for_vocab)))\n",
    "\n",
    "# Add special tokens to the word list\n",
    "words = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + unique_words_from_data # Changed from chars\n",
    "word_to_int = {word: i for i, word in enumerate(words)} # Changed from char_to_int\n",
    "int_to_word = {i: word for i, word in enumerate(words)} # Changed from int_to_char\n",
    "\n",
    "vocab_size = len(words) # Changed from len(chars)\n",
    "print(f\"Word Vocabulary Size (including SOS/EOS/UNK): {vocab_size}\")\n",
    "\n",
    "embedding_dim = 64 # tunable\n",
    "hidden_size = 128  # tunable\n",
    "output_size = vocab_size # Output size must match new vocab_size\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.0001 #trying shorter learning rate\n",
    "#sequence_length = 25 # Length of input sequences (now in words)\n",
    "sequence_length = 15 # trying a shorter sequence length\n",
    "epochs = 100 # Number of epochs\n",
    "dropout_probability = 0.5\n",
    "\n",
    "# Prepare training data\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for joke_str in raw_jokes:\n",
    "    # Tokenize joke into words, add SOS/EOS, and convert to lowercase\n",
    "    tokenized_joke_words = [SOS_TOKEN] + word_tokenize(joke_str.lower()) + [EOS_TOKEN]\n",
    "    \n",
    "    # Convert words to integers, using UNK_TOKEN for out-of-vocabulary words\n",
    "    # (though all words from raw_jokes should be in vocab here as it's built from them)\n",
    "    int_joke = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_joke_words]\n",
    "    \n",
    "    # Create sequences from this single joke\n",
    "    # A sequence needs at least sequence_length + 1 tokens (words) to form one input-output pair\n",
    "    if len(int_joke) > sequence_length:\n",
    "        for i in range(0, len(int_joke) - sequence_length, 1):\n",
    "            seq_in = int_joke[i : i + sequence_length]\n",
    "            seq_out = int_joke[i + sequence_length] # The target word (or EOS_TOKEN)\n",
    "            dataX.append(seq_in)\n",
    "            dataY.append(seq_out)\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns) # This will likely change\n",
    "\n",
    "split_idx = int(n_patterns * 0.8)\n",
    "\n",
    "# train_dataX = dataX[:split_idx]\n",
    "# train_dataY = dataY[:split_idx]\n",
    "# test_dataX = dataX[split_idx:]\n",
    "# test_dataY = dataY[split_idx:]\n",
    "\n",
    "# X_train = torch.tensor(train_dataX, dtype=torch.long)\n",
    "# Y_train = torch.tensor(train_dataY, dtype=torch.long)\n",
    "# X_test = torch.tensor(test_dataX, dtype=torch.long)\n",
    "# Y_test = torch.tensor(test_dataY, dtype=torch.long)\n",
    "\n",
    "# trying to use all data for training - getting rid of val and seeing what happens\n",
    "\n",
    "\n",
    "X_train = torch.tensor(dataX, dtype=torch.long)\n",
    "Y_train = torch.tensor(dataY, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Instantiate the model - this will now use the updated vocab_size\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=dropout_probability)\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# val_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False) # commenting out val for now\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# The training loop would typically be in your main script (e.g., main.ipynb)\n",
    "# or called from there if this helper.py is imported.\n",
    "# For completeness, if you intend to run training directly from this script,\n",
    "# you would add the training loop here:\n",
    "#\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # added gradient clipping\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase - commenting out for now\n",
    "    # model.eval()\n",
    "    # val_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_X_val, batch_y_val in val_loader:\n",
    "    #         outputs_val = model(batch_X_val)\n",
    "    #         loss_val = criterion(outputs_val, batch_y_val)\n",
    "    #         val_loss += loss_val.item()\n",
    "    \n",
    "    # avg_val_loss = val_loss / len(val_loader)\n",
    "    # print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2ba957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8922, 8922)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX), len(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ecad334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \"what do you call a\"\n",
      "Generated text: \n",
      "--------------------------\n",
      "what do you call a disease ? a pigment !  <EOS>\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Generation complete.\n",
      "Full generated joke:  what do you call a disease ? a pigment !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate text\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Parameters for generation\n",
    "start_string = \"What do you call a\" # Or any other seed text\n",
    "num_words_to_generate = 50 # Maximum number of words to generate\n",
    "temperature = 0.8 # Higher temperature results in more random, lower in more predictable text\n",
    "\n",
    "# Tokenize the start string and convert to integers\n",
    "tokenized_start_string = word_tokenize(start_string.lower())\n",
    "pattern = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_start_string]\n",
    "\n",
    "if not pattern:\n",
    "    print(f\"Error: Seed string '{start_string}' contains no known words or is too short after tokenization.\")\n",
    "    # Fallback to a default known pattern if the seed is problematic\n",
    "    # Using SOS_TOKEN as a starting point if the original start_string is empty or all unknown\n",
    "    pattern = [word_to_int[SOS_TOKEN]] * min(sequence_length, 5) # Use a short sequence of SOS_TOKEN\n",
    "    # Reconstruct start_string for printing purposes, though it might just be SOS tokens\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "    print(f\"Using fallback seed: '{start_string_for_print}'\")\n",
    "else:\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "\n",
    "\n",
    "generated_words = [int_to_word.get(p, UNK_TOKEN) for p in pattern] # Store generated words\n",
    "\n",
    "print(f\"Seed: \\\"{start_string_for_print}\\\"\")\n",
    "print(\"Generated text: \")\n",
    "print(\"--------------------------\")\n",
    "print(start_string_for_print, end=\" \") # Print with a space at the end\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for i in range(num_words_to_generate):\n",
    "        # Ensure the pattern is of the correct sequence_length for input\n",
    "        # If pattern is shorter than sequence_length, pad with UNK_TOKEN (or SOS_TOKEN)\n",
    "        # For simplicity, we'll just take the last part if it's long enough,\n",
    "        # or the whole pattern if it's shorter.\n",
    "        # The model expects sequence_length inputs.\n",
    "        \n",
    "        current_sequence_input_indices = pattern[-sequence_length:]\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = torch.tensor([current_sequence_input_indices], dtype=torch.long)\n",
    "        # input_tensor = input_tensor.to(device) # Uncomment if using GPU\n",
    "\n",
    "        # Get model output (logits)\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Apply temperature to logits\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        # Sample from the distribution\n",
    "        predicted_word_index = torch.multinomial(output_dist, 1)[0].item()\n",
    "        \n",
    "        # Get the predicted word\n",
    "        predicted_word = int_to_word.get(predicted_word_index, UNK_TOKEN)\n",
    "\n",
    "        if predicted_word == EOS_TOKEN:\n",
    "            print(f\" {EOS_TOKEN}\") # Print EOS token\n",
    "            break # Stop generation if EOS token is predicted\n",
    "        \n",
    "        generated_words.append(predicted_word)\n",
    "        pattern.append(predicted_word_index)\n",
    "        print(predicted_word, end=\" \", flush=True) # Print word with a space\n",
    "        \n",
    "        # Slide the window: ensure pattern doesn't grow indefinitely beyond what's needed for context\n",
    "        if len(pattern) > sequence_length:\n",
    "            pattern = pattern[1:]\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------\")\n",
    "print(\"\\nGeneration complete.\")\n",
    "\n",
    "# Full generated text:\n",
    "print(\"Full generated joke: \", \" \".join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730261d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448a7fd-05c4-438e-b757-7404517b1c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
