{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd84386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "download(\"punkt_tab\")\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37856106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/atticus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What did the bartender say to the jumper cable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't you hate jokes about German sausage? The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Two artists had an art contest... It ended in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Why did the chicken cross the playground? To g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What gun do you use to hunt a moose? A moosecut!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  What did the bartender say to the jumper cable...\n",
       "1   2  Don't you hate jokes about German sausage? The...\n",
       "2   3  Two artists had an art contest... It ended in ...\n",
       "3   4  Why did the chicken cross the playground? To g...\n",
       "4   5   What gun do you use to hunt a moose? A moosecut!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, download # Ensure nltk is imported\n",
    "\n",
    "# It's good practice to call nltk.download() at a more global scope or ensure it's run once.\n",
    "# These lines are already in your file, ensure 'punkt' is available for word_tokenize.\n",
    "download(\"punkt_tab\") \n",
    "download(\"stopwords\") # Not used in this specific change but kept from original\n",
    "\n",
    "df = pd.read_csv('../stage_4_data/text_generation/data.csv') # I changed it to data.csv\n",
    "#df = pd.read_csv('../stage_4_data/data.csv') # just for my directory structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93d092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vocabulary Size (including SOS/EOS/UNK): 4731\n",
      "Total Patterns:  8922\n",
      "Epoch [1/300], Training Loss: 8.4236\n",
      "Epoch [1/300], Validation Loss: 8.3465\n",
      "Epoch [2/300], Training Loss: 7.4861\n",
      "Epoch [2/300], Validation Loss: 6.2460\n",
      "Epoch [3/300], Training Loss: 5.8851\n",
      "Epoch [3/300], Validation Loss: 5.8896\n",
      "Epoch [4/300], Training Loss: 5.7062\n",
      "Epoch [4/300], Validation Loss: 5.8679\n",
      "Epoch [5/300], Training Loss: 5.6461\n",
      "Epoch [5/300], Validation Loss: 5.8667\n",
      "Epoch [6/300], Training Loss: 5.5892\n",
      "Epoch [6/300], Validation Loss: 5.8645\n",
      "Epoch [7/300], Training Loss: 5.5614\n",
      "Epoch [7/300], Validation Loss: 5.8660\n",
      "Epoch [8/300], Training Loss: 5.5087\n",
      "Epoch [8/300], Validation Loss: 5.8683\n",
      "Epoch [9/300], Training Loss: 5.4876\n",
      "Epoch [9/300], Validation Loss: 5.8746\n",
      "Epoch [10/300], Training Loss: 5.4438\n",
      "Epoch [10/300], Validation Loss: 5.8694\n",
      "Epoch [11/300], Training Loss: 5.4275\n",
      "Epoch [11/300], Validation Loss: 5.8679\n",
      "Epoch [12/300], Training Loss: 5.4086\n",
      "Epoch [12/300], Validation Loss: 5.8737\n",
      "Epoch [13/300], Training Loss: 5.3982\n",
      "Epoch [13/300], Validation Loss: 5.8726\n",
      "Epoch [14/300], Training Loss: 5.3747\n",
      "Epoch [14/300], Validation Loss: 5.8720\n",
      "Epoch [15/300], Training Loss: 5.3552\n",
      "Epoch [15/300], Validation Loss: 5.8753\n",
      "Epoch [16/300], Training Loss: 5.3405\n",
      "Epoch [16/300], Validation Loss: 5.8584\n",
      "Epoch [17/300], Training Loss: 5.3238\n",
      "Epoch [17/300], Validation Loss: 5.8593\n",
      "Epoch [18/300], Training Loss: 5.3023\n",
      "Epoch [18/300], Validation Loss: 5.8565\n",
      "Epoch [19/300], Training Loss: 5.2868\n",
      "Epoch [19/300], Validation Loss: 5.8607\n",
      "Epoch [20/300], Training Loss: 5.2702\n",
      "Epoch [20/300], Validation Loss: 5.8548\n",
      "Epoch [21/300], Training Loss: 5.2619\n",
      "Epoch [21/300], Validation Loss: 5.8523\n",
      "Epoch [22/300], Training Loss: 5.2512\n",
      "Epoch [22/300], Validation Loss: 5.8492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m     loss.backward()\n\u001b[32m    113\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m) \u001b[38;5;66;03m# added gradient clipping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     train_loss_epoch += loss.item()\n\u001b[32m    116\u001b[39m avg_train_loss = train_loss_epoch / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    211\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    213\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    214\u001b[39m         group,\n\u001b[32m    215\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         state_steps,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    782\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/189G/lib/python3.12/site-packages/torch/optim/adam.py:378\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    375\u001b[39m     param = torch.view_as_real(param)\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        #self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Added dropout layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out) # Apply dropout to the last time step\n",
    "        out = self.fc(out[:, -1, :]) # Get output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define special tokens\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\" # Token for unknown words\n",
    "\n",
    "# Get jokes from DataFrame\n",
    "raw_jokes = df['Joke'].astype(str).tolist()\n",
    "\n",
    "# Create word vocabulary\n",
    "all_words_for_vocab = []\n",
    "for joke_str in raw_jokes:\n",
    "    tokens = word_tokenize(joke_str.lower()) # Tokenize and convert to lowercase\n",
    "    all_words_for_vocab.extend(tokens)\n",
    "\n",
    "unique_words_from_data = sorted(list(set(all_words_for_vocab)))\n",
    "\n",
    "# Add special tokens to the word list\n",
    "words = [SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + unique_words_from_data # Changed from chars\n",
    "word_to_int = {word: i for i, word in enumerate(words)} # Changed from char_to_int\n",
    "int_to_word = {i: word for i, word in enumerate(words)} # Changed from int_to_char\n",
    "\n",
    "vocab_size = len(words) # Changed from len(chars)\n",
    "print(f\"Word Vocabulary Size (including SOS/EOS/UNK): {vocab_size}\")\n",
    "\n",
    "embedding_dim = 64 # tunable\n",
    "hidden_size = 128  # tunable\n",
    "output_size = vocab_size # Output size must match new vocab_size\n",
    "# learning_rate = 0.001\n",
    "learning_rate = 0.0001 #trying shorter learning rate\n",
    "#sequence_length = 25 # Length of input sequences (now in words)\n",
    "sequence_length = 15 # trying a shorter sequence length\n",
    "epochs = 300 # Number of epochs\n",
    "dropout_probability = 0.5\n",
    "\n",
    "# Prepare training data\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for joke_str in raw_jokes:\n",
    "    # Tokenize joke into words, add SOS/EOS, and convert to lowercase\n",
    "    tokenized_joke_words = [SOS_TOKEN] + word_tokenize(joke_str.lower()) + [EOS_TOKEN]\n",
    "    \n",
    "    # Convert words to integers, using UNK_TOKEN for out-of-vocabulary words\n",
    "    # (though all words from raw_jokes should be in vocab here as it's built from them)\n",
    "    int_joke = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_joke_words]\n",
    "    \n",
    "    # Create sequences from this single joke\n",
    "    # A sequence needs at least sequence_length + 1 tokens (words) to form one input-output pair\n",
    "    if len(int_joke) > sequence_length:\n",
    "        for i in range(0, len(int_joke) - sequence_length, 1):\n",
    "            seq_in = int_joke[i : i + sequence_length]\n",
    "            seq_out = int_joke[i + sequence_length] # The target word (or EOS_TOKEN)\n",
    "            dataX.append(seq_in)\n",
    "            dataY.append(seq_out)\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns) # This will likely change\n",
    "\n",
    "split_idx = int(n_patterns * 0.8)\n",
    "\n",
    "train_dataX = dataX[:split_idx]\n",
    "train_dataY = dataY[:split_idx]\n",
    "test_dataX = dataX[split_idx:]\n",
    "test_dataY = dataY[split_idx:]\n",
    "\n",
    "X_train = torch.tensor(train_dataX, dtype=torch.long)\n",
    "Y_train = torch.tensor(train_dataY, dtype=torch.long)\n",
    "X_test = torch.tensor(test_dataX, dtype=torch.long)\n",
    "Y_test = torch.tensor(test_dataY, dtype=torch.long)\n",
    "\n",
    "# Instantiate the model - this will now use the updated vocab_size\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size, output_size, dropout_prob=dropout_probability)\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# The training loop would typically be in your main script (e.g., main.ipynb)\n",
    "# or called from there if this helper.py is imported.\n",
    "# For completeness, if you intend to run training directly from this script,\n",
    "# you would add the training loop here:\n",
    "#\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # added gradient clipping\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            outputs_val = model(batch_X_val)\n",
    "            loss_val = criterion(outputs_val, batch_y_val)\n",
    "            val_loss += loss_val.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ee0392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \"why did\"\n",
      "Generated text: \n",
      "--------------------------\n",
      "why did joke in b edition cops .  <EOS>\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Generation complete.\n",
      "Full generated joke:  why did joke in b edition cops .\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Parameters for generation\n",
    "start_string = \"Why did\" # Or any other seed text\n",
    "num_words_to_generate = 50 # Maximum number of words to generate\n",
    "temperature = 0.8 # Higher temperature results in more random, lower in more predictable text\n",
    "\n",
    "# Tokenize the start string and convert to integers\n",
    "tokenized_start_string = word_tokenize(start_string.lower())\n",
    "pattern = [word_to_int.get(word, word_to_int[UNK_TOKEN]) for word in tokenized_start_string]\n",
    "\n",
    "if not pattern:\n",
    "    print(f\"Error: Seed string '{start_string}' contains no known words or is too short after tokenization.\")\n",
    "    # Fallback to a default known pattern if the seed is problematic\n",
    "    # Using SOS_TOKEN as a starting point if the original start_string is empty or all unknown\n",
    "    pattern = [word_to_int[SOS_TOKEN]] * min(sequence_length, 5) # Use a short sequence of SOS_TOKEN\n",
    "    # Reconstruct start_string for printing purposes, though it might just be SOS tokens\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "    print(f\"Using fallback seed: '{start_string_for_print}'\")\n",
    "else:\n",
    "    start_string_for_print = \" \".join([int_to_word.get(p, UNK_TOKEN) for p in pattern])\n",
    "\n",
    "\n",
    "generated_words = [int_to_word.get(p, UNK_TOKEN) for p in pattern] # Store generated words\n",
    "\n",
    "print(f\"Seed: \\\"{start_string_for_print}\\\"\")\n",
    "print(\"Generated text: \")\n",
    "print(\"--------------------------\")\n",
    "print(start_string_for_print, end=\" \") # Print with a space at the end\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for i in range(num_words_to_generate):\n",
    "        # Ensure the pattern is of the correct sequence_length for input\n",
    "        # If pattern is shorter than sequence_length, pad with UNK_TOKEN (or SOS_TOKEN)\n",
    "        # For simplicity, we'll just take the last part if it's long enough,\n",
    "        # or the whole pattern if it's shorter.\n",
    "        # The model expects sequence_length inputs.\n",
    "        \n",
    "        current_sequence_input_indices = pattern[-sequence_length:]\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = torch.tensor([current_sequence_input_indices], dtype=torch.long)\n",
    "        # input_tensor = input_tensor.to(device) # Uncomment if using GPU\n",
    "\n",
    "        # Get model output (logits)\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Apply temperature to logits\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        # Sample from the distribution\n",
    "        predicted_word_index = torch.multinomial(output_dist, 1)[0].item()\n",
    "        \n",
    "        # Get the predicted word\n",
    "        predicted_word = int_to_word.get(predicted_word_index, UNK_TOKEN)\n",
    "\n",
    "        if predicted_word == EOS_TOKEN:\n",
    "            print(f\" {EOS_TOKEN}\") # Print EOS token\n",
    "            break # Stop generation if EOS token is predicted\n",
    "        \n",
    "        generated_words.append(predicted_word)\n",
    "        pattern.append(predicted_word_index)\n",
    "        print(predicted_word, end=\" \", flush=True) # Print word with a space\n",
    "        \n",
    "        # Slide the window: ensure pattern doesn't grow indefinitely beyond what's needed for context\n",
    "        if len(pattern) > sequence_length:\n",
    "            pattern = pattern[1:]\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------\")\n",
    "print(\"\\nGeneration complete.\")\n",
    "\n",
    "# Full generated text:\n",
    "print(\"Full generated joke: \", \" \".join(generated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0146ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47fdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "189G",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
