{
 "cells": [
  {
   "cell_type": "code",
   "id": "1c0236cb2c248c6e",
   "metadata": {
    "id": "1c0236cb2c248c6e",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:39:31.201868Z",
     "start_time": "2025-05-25T01:39:17.253985Z"
    }
   },
   "source": [
    "# define import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a258d92701d9161e",
   "metadata": {
    "id": "a258d92701d9161e"
   },
   "source": [
    "# Data Loading Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "outputId": "8c4192b6-cc26-4e8c-e3ee-4acc2e88b1fe",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:39:32.717896Z",
     "start_time": "2025-05-25T01:39:31.242056Z"
    }
   },
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with open(\"text_classification_train_words\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(\"text_classification_test_words\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(f\"{len(train)} samples in train\")\n",
    "print(f\"{train[random.randint(0, len(train) - 1)]}\")\n",
    "print(f\"{len(test)} samples in test\")\n",
    "print(f\"{test[random.randint(0, len(test) - 1)]}\")\n",
    "print(len(train[0]['text']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 samples in train\n",
      "{'id': '377', 'rating': 1, 'label': 0, 'text': 'Don\\'t even ask me why I watched this! The only excuse I can come up with that I was sick with Bronchitis and too weak to change the channel. :) It\\'s too terrible for words, the movie that is, not the Bronchitis. The acting is deplorable, Richard Grieco hams it up as a trigger-happy, gun-slinging serial killer with a penchant for knocking off cops. Nick Mancuso phones in a performance as the cop on his trail and Nancy Allen manages to put in the only sympathetic role in the entire film. The script is dismal, peppered with clichéd lines, \"Are you ready, Pardner?\" purrs Richard Grieco to every single one of his victims. Dire. Avoid.', 'words': ['nt', 'even', 'ask', 'watched', 'excuse', 'come', 'sick', 'bronchitis', 'weak', 'change', 'channel', 'terrible', 'words', 'movie', 'bronchitis', 'acting', 'deplorable', 'richard', 'grieco', 'hams', 'triggerhappy', 'gunslinging', 'serial', 'killer', 'penchant', 'knocking', 'cops', 'nick', 'mancuso', 'phones', 'performance', 'cop', 'trail', 'nancy', 'allen', 'manages', 'put', 'sympathetic', 'role', 'entire', 'film', 'script', 'dismal', 'peppered', 'clichéd', 'lines', 'ready', 'pardner', 'purrs', 'richard', 'grieco', 'every', 'single', 'one', 'victims', 'dire', 'avoid']}\n",
      "25000 samples in test\n",
      "{'id': '3777', 'rating': 10, 'label': 1, 'text': \"Mark Blankfield played Jekyll and Hyde.<br /><br />Michael McGuire was the dad.<br /><br />Tim Thomerson was the plastic surgeon.<br /><br />Did you even see this movie? I doubt it!<br /><br />Blankfield was fairly popular at this time for playing the pill-popping doctor on Fridays. Thomerson has been funny in anything he does, from movies to series to stand-up comedy. If I ever find this movie on DVD I will definitely buy it. I recorded this movie off of HBO back in '82 and have pretty much worn out the tape. One of the funniest takes on the Jekyll & Hyde theme ever.<br /><br />Of course. with all the cocaine references in this movie, it'd be panned as being way too politically incorrect today, as would Cheech and Chong. Too bad, because it is FUNNY, FUNNY, FUNNY!\", 'words': ['mark', 'blankfield', 'played', 'jekyll', 'hyde', 'br', 'br', 'michael', 'mcguire', 'dad', 'br', 'br', 'tim', 'thomerson', 'plastic', 'surgeon', 'br', 'br', 'even', 'see', 'movie', 'doubt', 'br', 'br', 'blankfield', 'fairly', 'popular', 'time', 'playing', 'pillpopping', 'doctor', 'fridays', 'thomerson', 'funny', 'anything', 'movies', 'series', 'standup', 'comedy', 'ever', 'find', 'movie', 'dvd', 'definitely', 'buy', 'recorded', 'movie', 'hbo', 'back', 'pretty', 'much', 'worn', 'tape', 'one', 'funniest', 'takes', 'jekyll', 'hyde', 'theme', 'ever', 'br', 'br', 'course', 'cocaine', 'references', 'movie', 'panned', 'way', 'politically', 'incorrect', 'today', 'would', 'cheech', 'chong', 'bad', 'funny', 'funny', 'funny']}\n",
      "655\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f61adfab216d51ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f61adfab216d51ba",
    "outputId": "d3a9b945-1a9f-48a1-d623-64c56abb3d29",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:39:35.283849Z",
     "start_time": "2025-05-25T01:39:33.459016Z"
    }
   },
   "source": [
    "# get vocab size\n",
    "vocab = set()\n",
    "i = 0\n",
    "for sample in train + test:\n",
    "    for word in sample['words']:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Found a vocab size of {vocab_size}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a vocab size of 133264\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "422f662087b7a7aa",
   "metadata": {
    "id": "422f662087b7a7aa"
   },
   "source": [
    "## Prepare to be embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "d43f1d11acce111c",
   "metadata": {
    "id": "d43f1d11acce111c",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:39:35.720491Z",
     "start_time": "2025-05-25T01:39:35.507022Z"
    }
   },
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word_to_ix[word] = i"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "65934447f34331d2",
   "metadata": {
    "id": "65934447f34331d2"
   },
   "source": [
    "##  Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "18407271bb55f5eb",
   "metadata": {
    "id": "18407271bb55f5eb",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:40:46.506505Z",
     "start_time": "2025-05-25T01:39:36.100931Z"
    }
   },
   "source": [
    "def create_dataset(dataset):\n",
    "    X, Y = [], []\n",
    "    for data in dataset:\n",
    "        embeddings = torch.tensor([torch.tensor([word_to_ix[word]], dtype=torch.long) for word in data['words']])\n",
    "        X.append(embeddings)\n",
    "        Y.append(data['label'])\n",
    "\n",
    "    return pad_sequence(X, batch_first=True), torch.tensor(Y)\n",
    "\n",
    "\n",
    "X_train, Y_train = create_dataset(train)\n",
    "X_test, Y_test = create_dataset(test)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d7e29aaf5421f085",
   "metadata": {
    "id": "d7e29aaf5421f085"
   },
   "source": [
    "# Prepare For Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "beccbd9c58e6f651",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beccbd9c58e6f651",
    "outputId": "e2a221ce-08a0-4fe5-cdd7-9ffcc4f98280",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:40:47.474907Z",
     "start_time": "2025-05-25T01:40:46.979223Z"
    }
   },
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Func",
   "id": "9c9b0f8007219ca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:58:54.601903Z",
     "start_time": "2025-05-25T01:58:54.575543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, epochs):\n",
    "    # Instantiate the model\n",
    "    train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "    val_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        num_batches = len(train_loader)\n",
    "        for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "            print(f\"Batch [{i}/{num_batches}]\\r\", end=\"\")\n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}\\r')\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, batch_y_val in val_loader:\n",
    "                batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device) # Uncomment if using GPU\n",
    "                outputs_val = model(batch_X_val)\n",
    "                loss_val = criterion(outputs_val, batch_y_val)\n",
    "                val_loss += loss_val.item()\n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    print(\"Training complete.\")"
   ],
   "id": "edf6db619d5e2709",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RNN Model",
   "id": "8847bc1931cb8cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T02:08:39.162622Z",
     "start_time": "2025-05-25T02:08:39.142536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define our model class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 2)\n",
    "        # self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = torch.mean(out, dim=1) # take mean across time dimension\n",
    "        out = self.fc1(out)  # Get output from all time steps\n",
    "        # out = self.fc2(out)\n",
    "        return out"
   ],
   "id": "8f078f2751686ca7",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T02:14:31.297165Z",
     "start_time": "2025-05-25T02:12:53.053303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 64  # tunable\n",
    "hidden_size = 128  # tunable\n",
    "output_size = 2 # binary classification\n",
    "learning_rate = 0.001\n",
    "sequence_length = 100  # Length of input sequences\n",
    "epochs = 2  # Number of epochs\n",
    "\n",
    "# Instantiate the model\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "train_model(model, 10)"
   ],
   "id": "dfa011442430425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.6944\r\n",
      "Epoch [1/10], Validation Loss: 0.6919\n",
      "Epoch [2/10], Training Loss: 0.6566\r\n",
      "Epoch [2/10], Validation Loss: 0.5768\n",
      "Epoch [3/10], Training Loss: 0.6583\r\n",
      "Epoch [3/10], Validation Loss: 0.6897\n",
      "Epoch [4/10], Training Loss: 0.6049\r\n",
      "Epoch [4/10], Validation Loss: 0.5684\n",
      "Epoch [5/10], Training Loss: 0.5870\r\n",
      "Epoch [5/10], Validation Loss: 0.5360\n",
      "Epoch [6/10], Training Loss: 0.4766\r\n",
      "Epoch [6/10], Validation Loss: 0.4713\n",
      "Epoch [7/10], Training Loss: 0.4111\r\n",
      "Epoch [7/10], Validation Loss: 0.5421\n",
      "Epoch [8/10], Training Loss: 0.5149\r\n",
      "Epoch [8/10], Validation Loss: 0.4882\n",
      "Epoch [9/10], Training Loss: 0.4931\r\n",
      "Epoch [9/10], Validation Loss: 0.5252\n",
      "Epoch [10/10], Training Loss: 0.3939\r\n",
      "Epoch [10/10], Validation Loss: 0.5312\n",
      "Training complete.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T02:16:13.155390Z",
     "start_time": "2025-05-25T02:16:10.338730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate accuracy on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Create a DataLoader for the test set\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, Y_test), batch_size=64, shuffle=False)\n",
    "    for batch_X_test, batch_y_test in test_loader:\n",
    "        batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "        outputs_test = model(batch_X_test)\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "        total += batch_y_test.size(0)\n",
    "        correct += (predicted == batch_y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')"
   ],
   "id": "59f0c6e8746366f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test data: 81.30%\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM Model",
   "id": "99b4e4f2ce14aa81"
  },
  {
   "cell_type": "code",
   "id": "e2323b225572426c",
   "metadata": {
    "id": "e2323b225572426c",
    "ExecuteTime": {
     "end_time": "2025-05-25T01:45:38.708040Z",
     "start_time": "2025-05-25T01:45:38.690726Z"
    }
   },
   "source": [
    "# define our model class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm1_hidden_size, lstm2_hidden_size, dense_hidden_size, output_size, dropout_p=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout_embed = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(embedding_dim,\n",
    "                             lstm1_hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        self.dropout_lstm1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(lstm1_hidden_size * 2,\n",
    "                             lstm2_hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        self.dropout_lstm2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc1 = nn.Linear(lstm2_hidden_size * 2, dense_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_fc1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc2 = nn.Linear(dense_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout_embed(x) # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        lstm1_out = self.dropout_lstm1(lstm1_out)\n",
    "\n",
    "        _, (hn_lstm2, cn_lstm2) = self.lstm2(lstm1_out)\n",
    "\n",
    "        hidden_combined = torch.cat((hn_lstm2[-2,:,:], hn_lstm2[-1,:,:]), dim=1)\n",
    "\n",
    "        out_fc1 = self.fc1(hidden_combined)\n",
    "        out_relu = self.relu(out_fc1)\n",
    "        out_dropout_fc1 = self.dropout_fc1(out_relu)\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.fc2(out_dropout_fc1) # Shape: (batch_size, output_size)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "181c717a6ab31f1e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14db5d657b1d5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T03:08:17.360296Z",
     "start_time": "2025-05-23T03:08:17.181673Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14db5d657b1d5f4",
    "outputId": "2b1a10c5-30ee-49ab-cc3d-2065614c1b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 25000\n",
      "Training subset size: 22500\n",
      "Validation subset size: 2500\n",
      "Number of trainable parameters: 34684098\n",
      "Epoch [1/20], Training Loss: 0.6199\n",
      "Epoch [1/20], Validation Loss: 0.6827\n",
      "Epoch [2/20], Training Loss: 0.4863\n",
      "Epoch [2/20], Validation Loss: 0.4151\n",
      "Epoch [3/20], Training Loss: 0.3312\n",
      "Epoch [3/20], Validation Loss: 0.3368\n",
      "Epoch [4/20], Training Loss: 0.2483\n",
      "Epoch [4/20], Validation Loss: 0.3790\n",
      "Epoch [5/20], Training Loss: 0.1846\n",
      "Epoch [5/20], Validation Loss: 0.3731\n",
      "Epoch [6/20], Training Loss: 0.1324\n",
      "Epoch [6/20], Validation Loss: 0.3778\n",
      "Epoch [7/20], Training Loss: 0.0931\n",
      "Epoch [7/20], Validation Loss: 0.3789\n",
      "Epoch [8/20], Training Loss: 0.0680\n",
      "Epoch [8/20], Validation Loss: 0.4354\n",
      "Epoch [9/20], Training Loss: 0.0463\n",
      "Epoch [9/20], Validation Loss: 0.5011\n",
      "Epoch [10/20], Training Loss: 0.0354\n",
      "Epoch [10/20], Validation Loss: 0.5169\n",
      "Epoch [11/20], Training Loss: 0.0317\n",
      "Epoch [11/20], Validation Loss: 0.5119\n",
      "Epoch [12/20], Training Loss: 0.0291\n",
      "Epoch [12/20], Validation Loss: 0.5449\n",
      "Epoch [13/20], Training Loss: 0.0245\n",
      "Epoch [13/20], Validation Loss: 0.5766\n",
      "Epoch [14/20], Training Loss: 0.0194\n",
      "Epoch [14/20], Validation Loss: 0.5903\n",
      "Epoch [15/20], Training Loss: 0.0202\n",
      "Epoch [15/20], Validation Loss: 0.5995\n",
      "Epoch [16/20], Training Loss: 0.0182\n",
      "Epoch [16/20], Validation Loss: 0.5729\n",
      "Epoch [17/20], Training Loss: 0.0139\n",
      "Epoch [17/20], Validation Loss: 0.6135\n",
      "Epoch [18/20], Training Loss: 0.0156\n",
      "Epoch [18/20], Validation Loss: 0.6408\n",
      "Epoch [19/20], Training Loss: 0.0097\n",
      "Epoch [19/20], Validation Loss: 0.7470\n",
      "Epoch [20/20], Training Loss: 0.0114\n",
      "Epoch [20/20], Validation Loss: 0.6971\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "lstm1_hidden_size = 128 \n",
    "lstm2_hidden_size = 64\n",
    "dense_hidden_size = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "output_size = 2 # binary classification (remains the same)\n",
    "learning_rate = 0.001 # Keep as is, or tune\n",
    "epochs = 20  # Keep as is, or tune\n",
    "\n",
    "# Instantiate the model with new parameters\n",
    "model = RNN(vocab_size,\n",
    "            embedding_dim,\n",
    "            lstm1_hidden_size,\n",
    "            lstm2_hidden_size,\n",
    "            dense_hidden_size,\n",
    "            output_size,\n",
    "            dropout_p=dropout_rate).to(device)\n",
    "\n",
    "# 1. Create your initial TensorDataset\n",
    "full_train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "\n",
    "# 2. Define the sizes for your training and validation sets\n",
    "total_size = len(full_train_data)\n",
    "train_size = int(0.9 * total_size)  # 90% for training\n",
    "val_size = total_size - train_size   # Remaining 10% for validation\n",
    "\n",
    "# 3. Split the dataset\n",
    "train_subset, val_subset = torch.utils.data.random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "# 4. Create DataLoaders for your training and validation sets\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False) # No need to shuffle validation data\n",
    "\n",
    "print(f\"Original dataset size: {total_size}\")\n",
    "print(f\"Training subset size: {len(train_subset)}\")\n",
    "print(f\"Validation subset size: {len(val_subset)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_params}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    num_batches = len(train_loader)\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "        print(f\"Batch [{i}/{num_batches}]\\r\", end=\"\")\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}\\r')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device) # Uncomment if using GPU\n",
    "            outputs_val = model(batch_X_val)\n",
    "            loss_val = criterion(outputs_val, batch_y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6s4NpdLGyHnk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6s4NpdLGyHnk",
    "outputId": "0ba362a8-70ca-4c26-deac-ad00cbf3512b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test data: 85.27%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Create a DataLoader for the test set\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, Y_test), batch_size=64, shuffle=False)\n",
    "    for batch_X_test, batch_y_test in test_loader:\n",
    "        batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
    "        outputs_test = model(batch_X_test)\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "        total += batch_y_test.size(0)\n",
    "        correct += (predicted == batch_y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
