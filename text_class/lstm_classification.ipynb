{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c0236cb2c248c6e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:57:45.438722Z",
          "start_time": "2025-05-23T02:57:45.424219Z"
        },
        "id": "1c0236cb2c248c6e"
      },
      "outputs": [],
      "source": [
        "# define import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a258d92701d9161e",
      "metadata": {
        "id": "a258d92701d9161e"
      },
      "source": [
        "# Data Loading Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:57:47.290226Z",
          "start_time": "2025-05-23T02:57:46.242865Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "8c4192b6-cc26-4e8c-e3ee-4acc2e88b1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000 samples in train\n",
            "{'id': '9235', 'rating': 4, 'label': 0, 'text': 'This woman never stops talking throughout the movie. She memorized every line, and delivered all without a bit of natural emotion. She also has a most uncharming lisp, and the pitch of her voice sounds like nails on a blackboard. This film has WAY too much Betsy Drake, and not enough Cary Grant, who carried what little was left of the film entirely on his own.', 'words': ['woman', 'never', 'stops', 'talking', 'throughout', 'movie', 'memorized', 'every', 'line', 'delivered', 'without', 'bit', 'natural', 'emotion', 'also', 'uncharming', 'lisp', 'pitch', 'voice', 'sounds', 'like', 'nails', 'blackboard', 'film', 'way', 'much', 'betsy', 'drake', 'enough', 'cary', 'grant', 'carried', 'little', 'left', 'film', 'entirely']}\n",
            "25000 samples in test\n",
            "{'id': '12250', 'rating': 10, 'label': 1, 'text': \"Not only do I think this was the best film of 1987, it's probably in my own amorphous list as one of the 10-20 best films I've ever seen. For whatever reason, I really connected with this movie, and it is one of the most personal films I had seen at that point in my life (I was 26). For better or worse, I strongly identified with the Holly Hunter character (and I'm a guy!). She plays an extremely bright, loyal and intense woman who couldn't figure out romantic relationships. There were so many things that she said in this movie that were things that I would say or have said to others in similar circumstances. And the ending of the movie I find to be so very, very sad.<br /><br />Obviously, this role was the big break for Holly Hunter. Clearly, I was not the only one to think so highly of it.\", 'words': ['think', 'best', 'film', 'probably', 'amorphous', 'list', 'one', 'best', 'films', 'ever', 'seen', 'whatever', 'reason', 'really', 'connected', 'movie', 'one', 'personal', 'films', 'seen', 'point', 'life', 'better', 'worse', 'strongly', 'identified', 'holly', 'hunter', 'character', 'guy', 'plays', 'extremely', 'bright', 'loyal', 'intense', 'woman', 'could', 'nt', 'figure', 'romantic', 'relationships', 'many', 'things', 'said', 'movie', 'things', 'would', 'say', 'said', 'others', 'similar', 'circumstances', 'ending', 'movie', 'find', 'sad', 'br', 'br', 'obviously', 'role', 'big', 'break', 'holly', 'hunter', 'clearly', 'one', 'think', 'highly']}\n",
            "655\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "with open(\"text_classification_train_words\", \"rb\") as f:\n",
        "    train = pickle.load(f)\n",
        "\n",
        "with open(\"text_classification_test_words\", \"rb\") as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "print(f\"{len(train)} samples in train\")\n",
        "print(f\"{train[random.randint(0, len(train) - 1)]}\")\n",
        "print(f\"{len(test)} samples in test\")\n",
        "print(f\"{test[random.randint(0, len(test) - 1)]}\")\n",
        "print(len(train[0]['text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f61adfab216d51ba",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:57:48.715556Z",
          "start_time": "2025-05-23T02:57:47.711770Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61adfab216d51ba",
        "outputId": "d3a9b945-1a9f-48a1-d623-64c56abb3d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found a vocab size of 133264\n"
          ]
        }
      ],
      "source": [
        "# get vocab size\n",
        "vocab = set()\n",
        "i = 0\n",
        "for sample in train + test:\n",
        "    for word in sample['words']:\n",
        "        vocab.add(word)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Found a vocab size of {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422f662087b7a7aa",
      "metadata": {
        "id": "422f662087b7a7aa"
      },
      "source": [
        "## Prepare to be embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d43f1d11acce111c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:57:49.209999Z",
          "start_time": "2025-05-23T02:57:49.147769Z"
        },
        "id": "d43f1d11acce111c"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "word_to_ix = {}\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    word_to_ix[word] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65934447f34331d2",
      "metadata": {
        "id": "65934447f34331d2"
      },
      "source": [
        "##  Split Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "18407271bb55f5eb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:58:19.071722Z",
          "start_time": "2025-05-23T02:57:50.388231Z"
        },
        "id": "18407271bb55f5eb"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset):\n",
        "    X, Y = [], []\n",
        "    for data in dataset:\n",
        "        embeddings = torch.tensor([torch.tensor([word_to_ix[word]], dtype=torch.long) for word in data['words']])\n",
        "        X.append(embeddings)\n",
        "        Y.append(data['label'])\n",
        "\n",
        "    return pad_sequence(X, batch_first=True), torch.tensor(Y)\n",
        "\n",
        "\n",
        "X_train, Y_train = create_dataset(train)\n",
        "X_test, Y_test = create_dataset(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e29aaf5421f085",
      "metadata": {
        "id": "d7e29aaf5421f085"
      },
      "source": [
        "# Prepare For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "beccbd9c58e6f651",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T02:57:43.393955Z",
          "start_time": "2025-05-23T02:57:43.336425Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beccbd9c58e6f651",
        "outputId": "e2a221ce-08a0-4fe5-cdd7-9ffcc4f98280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e2323b225572426c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T03:08:16.272948Z",
          "start_time": "2025-05-23T03:08:16.263053Z"
        },
        "id": "e2323b225572426c"
      },
      "outputs": [],
      "source": [
        "# define our model class\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, lstm1_hidden_size, lstm2_hidden_size, dense_hidden_size, output_size, dropout_p=0.5):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout_embed = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(embedding_dim,\n",
        "                             lstm1_hidden_size,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "        self.dropout_lstm1 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(lstm1_hidden_size * 2,\n",
        "                             lstm2_hidden_size,\n",
        "                             num_layers=1,\n",
        "                             batch_first=True,\n",
        "                             bidirectional=True)\n",
        "        self.dropout_lstm2 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc1 = nn.Linear(lstm2_hidden_size * 2, dense_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc1 = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Final output layer\n",
        "        self.fc2 = nn.Linear(dense_hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embed(x) # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        lstm1_out, _ = self.lstm1(x)\n",
        "        lstm1_out = self.dropout_lstm1(lstm1_out)\n",
        "\n",
        "        _, (hn_lstm2, cn_lstm2) = self.lstm2(lstm1_out)\n",
        "\n",
        "        hidden_combined = torch.cat((hn_lstm2[-2,:,:], hn_lstm2[-1,:,:]), dim=1)\n",
        "\n",
        "        out_fc1 = self.fc1(hidden_combined)\n",
        "        out_relu = self.relu(out_fc1)\n",
        "        out_dropout_fc1 = self.dropout_fc1(out_relu)\n",
        "\n",
        "        # Final output layer\n",
        "        out = self.fc2(out_dropout_fc1) # Shape: (batch_size, output_size)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "14db5d657b1d5f4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-23T03:08:17.360296Z",
          "start_time": "2025-05-23T03:08:17.181673Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14db5d657b1d5f4",
        "outputId": "2b1a10c5-30ee-49ab-cc3d-2065614c1b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset size: 25000\n",
            "Training subset size: 22500\n",
            "Validation subset size: 2500\n",
            "Number of trainable parameters: 34684098\n",
            "Epoch [1/20], Training Loss: 0.6199\n",
            "Epoch [1/20], Validation Loss: 0.6827\n",
            "Epoch [2/20], Training Loss: 0.4863\n",
            "Epoch [2/20], Validation Loss: 0.4151\n",
            "Epoch [3/20], Training Loss: 0.3312\n",
            "Epoch [3/20], Validation Loss: 0.3368\n",
            "Epoch [4/20], Training Loss: 0.2483\n",
            "Epoch [4/20], Validation Loss: 0.3790\n",
            "Epoch [5/20], Training Loss: 0.1846\n",
            "Epoch [5/20], Validation Loss: 0.3731\n",
            "Epoch [6/20], Training Loss: 0.1324\n",
            "Epoch [6/20], Validation Loss: 0.3778\n",
            "Epoch [7/20], Training Loss: 0.0931\n",
            "Epoch [7/20], Validation Loss: 0.3789\n",
            "Epoch [8/20], Training Loss: 0.0680\n",
            "Epoch [8/20], Validation Loss: 0.4354\n",
            "Epoch [9/20], Training Loss: 0.0463\n",
            "Epoch [9/20], Validation Loss: 0.5011\n",
            "Epoch [10/20], Training Loss: 0.0354\n",
            "Epoch [10/20], Validation Loss: 0.5169\n",
            "Epoch [11/20], Training Loss: 0.0317\n",
            "Epoch [11/20], Validation Loss: 0.5119\n",
            "Epoch [12/20], Training Loss: 0.0291\n",
            "Epoch [12/20], Validation Loss: 0.5449\n",
            "Epoch [13/20], Training Loss: 0.0245\n",
            "Epoch [13/20], Validation Loss: 0.5766\n",
            "Epoch [14/20], Training Loss: 0.0194\n",
            "Epoch [14/20], Validation Loss: 0.5903\n",
            "Epoch [15/20], Training Loss: 0.0202\n",
            "Epoch [15/20], Validation Loss: 0.5995\n",
            "Epoch [16/20], Training Loss: 0.0182\n",
            "Epoch [16/20], Validation Loss: 0.5729\n",
            "Epoch [17/20], Training Loss: 0.0139\n",
            "Epoch [17/20], Validation Loss: 0.6135\n",
            "Epoch [18/20], Training Loss: 0.0156\n",
            "Epoch [18/20], Validation Loss: 0.6408\n",
            "Epoch [19/20], Training Loss: 0.0097\n",
            "Epoch [19/20], Validation Loss: 0.7470\n",
            "Epoch [20/20], Training Loss: 0.0114\n",
            "Epoch [20/20], Validation Loss: 0.6971\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 256\n",
        "lstm1_hidden_size = 128 \n",
        "lstm2_hidden_size = 64\n",
        "dense_hidden_size = 64\n",
        "dropout_rate = 0.2\n",
        "\n",
        "output_size = 2 # binary classification (remains the same)\n",
        "learning_rate = 0.001 # Keep as is, or tune\n",
        "epochs = 20  # Keep as is, or tune\n",
        "\n",
        "# Instantiate the model with new parameters\n",
        "model = RNN(vocab_size,\n",
        "            embedding_dim,\n",
        "            lstm1_hidden_size,\n",
        "            lstm2_hidden_size,\n",
        "            dense_hidden_size,\n",
        "            output_size,\n",
        "            dropout_p=dropout_rate).to(device)\n",
        "\n",
        "# 1. Create your initial TensorDataset\n",
        "full_train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
        "\n",
        "# 2. Define the sizes for your training and validation sets\n",
        "total_size = len(full_train_data)\n",
        "train_size = int(0.9 * total_size)  # 90% for training\n",
        "val_size = total_size - train_size   # Remaining 10% for validation\n",
        "\n",
        "# 3. Split the dataset\n",
        "train_subset, val_subset = torch.utils.data.random_split(full_train_data, [train_size, val_size])\n",
        "\n",
        "# 4. Create DataLoaders for your training and validation sets\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False) # No need to shuffle validation data\n",
        "\n",
        "print(f\"Original dataset size: {total_size}\")\n",
        "print(f\"Training subset size: {len(train_subset)}\")\n",
        "print(f\"Validation subset size: {len(val_subset)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params}\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss_epoch = 0\n",
        "    num_batches = len(train_loader)\n",
        "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_epoch += loss.item()\n",
        "        print(f\"Batch [{i}/{num_batches}]\\r\", end=\"\")\n",
        "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}\\r')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X_val, batch_y_val in val_loader:\n",
        "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device) # Uncomment if using GPU\n",
        "            outputs_val = model(batch_X_val)\n",
        "            loss_val = criterion(outputs_val, batch_y_val)\n",
        "            val_loss += loss_val.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6s4NpdLGyHnk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s4NpdLGyHnk",
        "outputId": "0ba362a8-70ca-4c26-deac-ad00cbf3512b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test data: 85.27%\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Create a DataLoader for the test set\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, Y_test), batch_size=64, shuffle=False)\n",
        "    for batch_X_test, batch_y_test in test_loader:\n",
        "        batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n",
        "        outputs_test = model(batch_X_test)\n",
        "        _, predicted = torch.max(outputs_test.data, 1)\n",
        "        total += batch_y_test.size(0)\n",
        "        correct += (predicted == batch_y_test).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
