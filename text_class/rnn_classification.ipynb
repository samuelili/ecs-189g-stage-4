{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0236cb2c248c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:57:45.438722Z",
     "start_time": "2025-05-23T02:57:45.424219Z"
    }
   },
   "outputs": [],
   "source": [
    "# define import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258d92701d9161e",
   "metadata": {},
   "source": [
    "# Data Loading Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:57:47.290226Z",
     "start_time": "2025-05-23T02:57:46.242865Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 samples in train\n",
      "{'id': '4303', 'rating': 1, 'label': 0, 'text': \"Previous comment made me write this. It says that Muslims are blonde and Serbs are dark (because our blood is mixed). This comment just says that this opinion can be made by racist.Look,race is nothing.I'm color blind.I look like Pierce Brosnan but I'm no Irish. So what?I might add that I am not 100% Serb,that I have some Austrian and Croat blood within me but whats the point.I'm dark, half-breed?Is that so? Anyone using racial prejudices with such bad intent like Lantos(producer9and director is racist for me.Karadzhic, Izetbegovich, Milosevic, Tudjman they are all monsters and I blame them for destroying my life, my family, my country, Yuggoslavia. Hope they will be all in hell but that wont return our dead relatives back. I am proud of being Serb and I am proud of my cousins, Austrians,Croats,Muslims, Hungarians, Arabs (yes I am from Serbia and I have multiethnical family).This movie doesn't show sufferings of Serbs or Croats within Sarayevo,terrible terrorism of street gangs,Muslim extremism.I add: I kneel and pray for all innocent sisters and brothers Muslim,catholic or orthodox, killed in this war.This film is manipulation with our misery,false humanitarianism's which doesn't help at all.It helps Lantos to fill his pockets with more doe,alright!\", 'words': ['previous', 'comment', 'made', 'write', 'says', 'muslims', 'blonde', 'serbs', 'dark', 'blood', 'mixed', 'comment', 'says', 'opinion', 'made', 'racistlook', 'race', 'nothingi', 'color', 'blindi', 'look', 'like', 'pierce', 'brosnan', 'irish', 'might', 'add', 'serb', 'austrian', 'croat', 'blood', 'within', 'whats', 'pointi', 'dark', 'halfbreed', 'anyone', 'using', 'racial', 'prejudices', 'bad', 'intent', 'like', 'lantos', 'director', 'racist', 'mekaradzhic', 'izetbegovich', 'milosevic', 'tudjman', 'monsters', 'blame', 'destroying', 'life', 'family', 'country', 'yuggoslavia', 'hope', 'hell', 'wont', 'return', 'dead', 'relatives', 'back', 'proud', 'serb', 'proud', 'cousins', 'austrians', 'croats', 'muslims', 'hungarians', 'arabs', 'yes', 'serbia', 'multiethnical', 'family', 'movie', 'nt', 'show', 'sufferings', 'serbs', 'croats', 'within', 'sarayevo', 'terrible', 'terrorism', 'street', 'gangs', 'muslim', 'extremismi', 'add', 'kneel', 'pray', 'innocent', 'sisters', 'brothers', 'muslim', 'catholic', 'orthodox', 'killed', 'warthis', 'film', 'manipulation', 'misery', 'false', 'humanitarianism', 'nt', 'help', 'allit', 'helps', 'lantos', 'fill', 'pockets', 'doe', 'alright']}\n",
      "25000 samples in test\n",
      "{'id': '7136', 'rating': 3, 'label': 0, 'text': 'Aubrey Davis (Amber Tamblyn) travels to Tokyo to investigate the mysterious disappearance of her sister Karen (Sarah Michelle Gellar) and gets caught up in the same mysterious curse that has killed so many people. With a group of others, she tries to solve and end the curse for good.<br /><br />The Grudge 2 rehashes everything from the first one and it produces only a couple of scares. The first Grudge was an average movie at best. The premise was decent but the acting was wooden and some of the scenes were really ridiculous. The Grudge 2 is even more silly and less scary than the original. Also, Scary Movie 4 has made it nearly impossible to take these films seriously and I had trouble keeping a straight face during the \"scary scenes\" which should encourage laughter more than fear. How Takashi Shimizu messed up his own franchise is unexplainable yet he succeeded at creating a decent atmosphere. Unfortunately, he kept the film at a slow pace with a bunch of dull characters and lame scare tactics. Clearly, he did this movie for money and I\\'m glad it failed.<br /><br />The screenplay was pretty bad, mainly because it made no sense and they didn\\'t develop the characters at all. In the movie, there were three different stories going on and each of them were somehow related to \"the grudge\". The connections were weak and a bunch of people were killed without a real reason. They don\\'t explain the rules of the curse very well and it ended up being a bit of blood bath because of everyone dying although the PG-13 rating kept things from getting interesting so even some of the death scenes were pretty lame. The dialog was weak and none of the characters were likable or developed well enough to truly care about.<br /><br />The acting was on par with everything else and it was a lot worse compared to the first one. Amber Tamblyn was annoyingly wooden. She moved on screen very slowly and all of her emotions seemed fake. Sarah Michelle Gellar was okay although she just had a cameo. Jennifer Beals gave the best performance, she had a few good scenes and was kind of effective. Arielle Kebbel was okay, a little bland. The rest were either horrible or just too forgettable to mention. Overall, The Grudge 2 was a disappointing sequel. It lacked a lot of things and it\\'s not worth watching. Rating 4/10', 'words': ['aubrey', 'davis', 'amber', 'tamblyn', 'travels', 'tokyo', 'investigate', 'mysterious', 'disappearance', 'sister', 'karen', 'sarah', 'michelle', 'gellar', 'gets', 'caught', 'mysterious', 'curse', 'killed', 'many', 'people', 'group', 'others', 'tries', 'solve', 'end', 'curse', 'good', 'br', 'br', 'grudge', 'rehashes', 'everything', 'first', 'one', 'produces', 'couple', 'scares', 'first', 'grudge', 'average', 'movie', 'best', 'premise', 'decent', 'acting', 'wooden', 'scenes', 'really', 'ridiculous', 'grudge', 'even', 'silly', 'less', 'scary', 'original', 'also', 'scary', 'movie', 'made', 'nearly', 'impossible', 'take', 'films', 'seriously', 'trouble', 'keeping', 'straight', 'face', 'scary', 'scenes', 'encourage', 'laughter', 'fear', 'takashi', 'shimizu', 'messed', 'franchise', 'unexplainable', 'yet', 'succeeded', 'creating', 'decent', 'atmosphere', 'unfortunately', 'kept', 'film', 'slow', 'pace', 'bunch', 'dull', 'characters', 'lame', 'scare', 'tactics', 'clearly', 'movie', 'money', 'glad', 'failed', 'br', 'br', 'screenplay', 'pretty', 'bad', 'mainly', 'made', 'sense', 'nt', 'develop', 'characters', 'movie', 'three', 'different', 'stories', 'going', 'somehow', 'related', 'grudge', 'connections', 'weak', 'bunch', 'people', 'killed', 'without', 'real', 'reason', 'nt', 'explain', 'rules', 'curse', 'well', 'ended', 'bit', 'blood', 'bath', 'everyone', 'dying', 'although', 'rating', 'kept', 'things', 'getting', 'interesting', 'even', 'death', 'scenes', 'pretty', 'lame', 'dialog', 'weak', 'none', 'characters', 'likable', 'developed', 'well', 'enough', 'truly', 'care', 'br', 'br', 'acting', 'par', 'everything', 'else', 'lot', 'worse', 'compared', 'first', 'one', 'amber', 'tamblyn', 'annoyingly', 'wooden', 'moved', 'screen', 'slowly', 'emotions', 'seemed', 'fake', 'sarah', 'michelle', 'gellar', 'okay', 'although', 'cameo', 'jennifer', 'beals', 'gave', 'best', 'performance', 'good', 'scenes', 'kind', 'effective', 'arielle', 'kebbel', 'okay', 'little', 'bland', 'rest', 'either', 'horrible', 'forgettable', 'mention', 'overall', 'grudge', 'disappointing', 'sequel', 'lacked', 'lot', 'things', 'worth', 'watching', 'rating']}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with open(\"text_classification_train_words\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(\"text_classification_test_words\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(f\"{len(train)} samples in train\")\n",
    "print(f\"{train[random.randint(0, len(train) - 1)]}\")\n",
    "print(f\"{len(test)} samples in test\")\n",
    "print(f\"{test[random.randint(0, len(test) - 1)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61adfab216d51ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:57:48.715556Z",
     "start_time": "2025-05-23T02:57:47.711770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a vocab size of 133264\n"
     ]
    }
   ],
   "source": [
    "# get vocab size\n",
    "vocab = set()\n",
    "i = 0\n",
    "for sample in train + test:\n",
    "    for word in sample['words']:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Found a vocab size of {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f662087b7a7aa",
   "metadata": {},
   "source": [
    "## Prepare to be embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43f1d11acce111c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:57:49.209999Z",
     "start_time": "2025-05-23T02:57:49.147769Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word_to_ix[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65934447f34331d2",
   "metadata": {},
   "source": [
    "##  Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18407271bb55f5eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:58:19.071722Z",
     "start_time": "2025-05-23T02:57:50.388231Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset):\n",
    "    X, Y = [], []\n",
    "    for data in dataset:\n",
    "        embeddings = torch.tensor([torch.tensor([word_to_ix[word]], dtype=torch.long) for word in data['words']])\n",
    "        X.append(embeddings)\n",
    "        Y.append(data['label'])\n",
    "\n",
    "    return pad_sequence(X, batch_first=True), torch.tensor(Y)\n",
    "\n",
    "\n",
    "X_train, Y_train = create_dataset(train)\n",
    "X_test, Y_test = create_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e29aaf5421f085",
   "metadata": {},
   "source": [
    "# Prepare For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beccbd9c58e6f651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:57:43.393955Z",
     "start_time": "2025-05-23T02:57:43.336425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2323b225572426c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T03:08:16.272948Z",
     "start_time": "2025-05-23T03:08:16.263053Z"
    }
   },
   "outputs": [],
   "source": [
    "# define our model class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, dropout_p=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout_embed = nn.Dropout(dropout_p)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.dropout_rnn_out = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout_embed(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        last_out = out[:, -1, :]\n",
    "        last_out = self.dropout_rnn_out(last_out)\n",
    "        out = self.fc(last_out)  # Get output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14db5d657b1d5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T03:08:17.360296Z",
     "start_time": "2025-05-23T03:08:17.181673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 0.6958\n",
      "Epoch [1/2], Validation Loss: 0.6957\n",
      "Epoch [2/2], Training Loss: 0.6949\n",
      "Epoch [2/2], Validation Loss: 0.6933\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64  # tunable\n",
    "hidden_size = 128  # tunable\n",
    "output_size = 2 # binary classification\n",
    "learning_rate = 0.001\n",
    "sequence_length = 100  # Length of input sequences\n",
    "epochs = 2  # Number of epochs\n",
    "\n",
    "# Instantiate the model\n",
    "model = RNN(vocab_size, embedding_dim, hidden_size, output_size).to(device)\n",
    "train_data = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    num_batches = len(train_loader)\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "        print(f\"Batch [{i}/{num_batches}]\\r\", end=\"\")\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}\\r')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader:\n",
    "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device) # Uncomment if using GPU\n",
    "            outputs_val = model(batch_X_val)\n",
    "            loss_val = criterion(outputs_val, batch_y_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
